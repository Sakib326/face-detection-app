<!DOCTYPE html>
<html>
  <head>
    <title>Face Recognition Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
      #video-container {
        position: relative;
        margin: 20px;
      }
      canvas {
        position: absolute;
        top: 0;
        left: 0;
      }
      #status {
        margin: 10px;
        padding: 10px;
        background-color: #f0f0f0;
        border-radius: 4px;
      }
      .error {
        color: red;
      }
      #controls {
        margin: 20px;
      }
      button {
        padding: 8px 16px;
        margin: 0 5px;
        background-color: #4caf50;
        color: white;
        border: none;
        border-radius: 4px;
        cursor: pointer;
      }
      button:hover {
        background-color: #45a049;
      }
      input {
        padding: 8px;
        margin-right: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
        width: 200px;
      }
    </style>
  </head>
  <body>
    <div id="status">Initializing...</div>

    <div id="controls">
      <input type="text" id="personName" placeholder="Enter person's name" />
      <button onclick="registerPerson()">Register Face</button>
      <button onclick="clearRegisteredFaces()">Clear All Faces</button>
    </div>

    <div id="video-container">
      <video id="video" width="640" height="480" autoplay muted></video>
    </div>

    <script>
      const video = document.getElementById("video");
      const status = document.getElementById("status");
      let labeledFaceDescriptors = [];

      function checkWebcamSupport() {
        return !!(
          navigator.mediaDevices && navigator.mediaDevices.getUserMedia
        );
      }

      async function checkModelsDirectory() {
        try {
          const response = await fetch(
            "./models/tiny_face_detector_model-weights_manifest.json"
          );
          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
          }
          return true;
        } catch (error) {
          status.innerHTML = `<span class="error">Error accessing models directory: ${error.message}<br>
                    Please ensure you're running through a web server and the models folder is in the correct location.</span>`;
          return false;
        }
      }

      async function loadModels() {
        try {
          if (!checkWebcamSupport()) {
            throw new Error("Webcam not supported in this browser");
          }

          status.textContent = "Checking models directory...";
          const modelsAccessible = await checkModelsDirectory();

          if (!modelsAccessible) return;

          status.textContent = "Loading face detector...";
          await faceapi.nets.tinyFaceDetector.loadFromUri("./models");

          status.textContent = "Loading landmark detection...";
          await faceapi.nets.faceLandmark68Net.loadFromUri("./models");

          status.textContent = "Loading recognition model...";
          await faceapi.nets.faceRecognitionNet.loadFromUri("./models");

          status.textContent = "Loading expression model...";
          await faceapi.nets.faceExpressionNet.loadFromUri("./models");

          status.textContent = "All models loaded! Starting video...";
          await startVideo();
        } catch (error) {
          status.innerHTML = `<span class="error">Error loading models: ${error.message}<br>
                    Check console for more details.</span>`;
          console.error("Detailed error:", error);
        }
      }

      async function startVideo() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: {
              width: 640,
              height: 480,
            },
          });
          video.srcObject = stream;
          status.textContent = "Video started! Face detection active.";
        } catch (err) {
          throw new Error(`Webcam error: ${err.message}`);
        }
      }

      async function registerPerson() {
        const personName = document.getElementById("personName").value.trim();
        if (!personName) {
          alert("Please enter a name");
          return;
        }

        try {
          status.textContent = "Detecting face for registration...";
          const detection = await faceapi
            .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptor();

          if (!detection) {
            alert(
              "No face detected! Please make sure your face is clearly visible."
            );
            return;
          }

          labeledFaceDescriptors.push({
            name: personName,
            descriptor: detection.descriptor,
          });

          status.textContent = `Registered ${personName}! Total registered faces: ${labeledFaceDescriptors.length}`;
          document.getElementById("personName").value = "";
        } catch (error) {
          status.innerHTML = `<span class="error">Registration error: ${error.message}</span>`;
          console.error("Registration error:", error);
        }
      }

      function clearRegisteredFaces() {
        labeledFaceDescriptors = [];
        status.textContent = "All registered faces cleared";
      }

      window.addEventListener("load", async () => {
        try {
          await loadModels();
        } catch (error) {
          console.error("Startup error:", error);
        }
      });

      video.addEventListener("play", () => {
        const canvas = faceapi.createCanvasFromMedia(video);
        document.getElementById("video-container").append(canvas);

        const displaySize = { width: video.width, height: video.height };
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async () => {
          try {
            const detections = await faceapi
              .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
              .withFaceLandmarks()
              .withFaceDescriptors()
              .withFaceExpressions();

            const resizedDetections = faceapi.resizeResults(
              detections,
              displaySize
            );
            canvas
              .getContext("2d")
              .clearRect(0, 0, canvas.width, canvas.height);

            // Process each detected face
            resizedDetections.forEach((detection) => {
              const box = detection.detection.box;
              let label = "Unknown";

              // Try to match with registered faces
              if (labeledFaceDescriptors.length > 0) {
                const match = labeledFaceDescriptors
                  .map((fd) => ({
                    name: fd.name,
                    distance: faceapi.euclideanDistance(
                      detection.descriptor,
                      fd.descriptor
                    ),
                  }))
                  .reduce((prev, curr) =>
                    prev.distance < curr.distance ? prev : curr
                  );

                if (match.distance < 0.5) {
                  label = match.name;
                }
              }

              // Draw box with name and expression
              const expressions = detection.expressions;
              const dominantExpression = Object.entries(expressions).reduce(
                (a, b) => (a[1] > b[1] ? a : b)
              )[0];

              const drawBox = new faceapi.draw.DrawBox(box, {
                label: `${label}\n${dominantExpression}`,
              });
              drawBox.draw(canvas);
            });

            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
          } catch (error) {
            console.error("Detection error:", error);
          }
        }, 100);
      });
    </script>
  </body>
</html>
